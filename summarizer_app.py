# -*- coding: utf-8 -*-
"""summarizer_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xVLWKI2mteHMHaYk-kVgS3HljnEzM8nP
"""

%pip install streamlit

%pip install streamlit requests beautifulsoup4 groq

import streamlit as st
import streamlit as st
import requests
from bs4 import BeautifulSoup
from groq import Groq
import datetime

st.set_page_config(page_title="Web Article Summarizer", layout="centered")

# ============================
# FUNGSI SCRAPING & SUMMARIZING
# ============================

groq_api_key = "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"
client = Groq(api_key=groq_api_key)

def get_cleaned_text_from_url(url: str) -> str:
    """
    Mengambil dan membersihkan isi teks dari halaman web.
    Hanya teks dari tag <p> yang digunakan untuk diringkas.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        return f"âŒ Gagal mengambil URL: {e}"

    soup = BeautifulSoup(response.text, "html.parser")

    # Hapus elemen tidak penting
    for tag in soup(["script", "style", "noscript", "header", "footer", "svg", "img"]):
        tag.decompose()

    # Gabungkan isi dari semua tag <p>
    text = "\n".join(p.get_text(strip=True) for p in soup.find_all("p"))
    return text.strip()

def summarize_text(text: str, max_tokens: int = 15000) -> str:
    """
    Merangkum teks menggunakan model LLM Groq.
    """
    try:
        response = client.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",  # Ganti model bila diperlukan
            messages=[
                {"role": "system", "content": "Kamu adalah asisten yang merangkum teks secara singkat, jelas, dan akurat."},
                {"role": "user", "content": f"Ringkas artikel berikut dengan gaya formal dan informatif:\n\n{text}"}
            ],
            temperature=0.5,
            top_p=1.0,
            stream=False,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ Gagal merangkum artikel: {e}"

def count_words(text: str) -> int:
    """
    Menghitung jumlah kata dalam teks.
    """
    return len(text.split())

def summarize_webpage(url: str) -> str:
    """
    Prosedur lengkap: scraping, menghitung kata, dan meringkas.
    """
    print(f"ğŸ” Mengambil konten dari: {url}")
    raw_text = get_cleaned_text_from_url(url)

    if raw_text.startswith("âŒ"):
        return raw_text

    num_chars = len(raw_text)
    num_words = count_words(raw_text)

    print(f"\nğŸ“Š Statistik Artikel:")
    print(f"   â€¢ Jumlah karakter: {num_chars}")
    print(f"   â€¢ Jumlah kata     : {num_words}")

    lanjut = input("\nâ“Ingin melanjutkan ringkasan? (y/n): ").strip().lower()
    if lanjut != 'y':
        return "â¹ï¸ Ringkasan dibatalkan oleh pengguna."

    return summarize_text(raw_text)

if __name__ == "__main__":
    print("ğŸ” Mode Multi-Artikel Aktif")
    print("Ketik 'selesai' atau 'exit' untuk berhenti.\n")

    while True:
        url_input = input("ğŸŒ Masukkan URL artikel: ").strip()
        if url_input.lower() in ["selesai", "exit", "stop", "quit"]:
            print("\nâœ… Sesi ringkasan selesai.")
            break

        hasil_ringkasan = summarize_webpage(url_input)
        print("\nğŸ“ Ringkasan Artikel:\n")
        print(hasil_ringkasan)
        print("\n" + "="*80 + "\n")

# ============================
# STREAMLIT APP
# ============================

st.title("ğŸ“š Web Article Summarizer")

# 1. Masukkan API Key
with st.expander("1ï¸âƒ£ Masukkan API Key Groq-mu"):
    user_api_key = st.text_input("ğŸ”‘ API Key kamu:", type="password")

# 2. Input URL dan Proses Ringkasan
if user_api_key:
    st.subheader("2ï¸âƒ£ Masukkan Link Web yang Ingin Dirangkum")
    url = st.text_input("ğŸŒ Masukkan URL artikel:", placeholder="https://example.com/artikel")

    if url:
        if st.button("ğŸ” Ambil & Ringkas Artikel"):
            with st.spinner("ğŸš€ Mengambil dan merangkum artikel..."):
                raw_text = get_cleaned_text_from_url(url)

                if raw_text.startswith("âŒ"):
                    st.error(raw_text)
                else:
                    word_count = count_words(raw_text)
                    char_count = len(raw_text)
                    st.info(f"ğŸ“Š Artikel memiliki {word_count} kata dan {char_count} karakter.")

                    ringkasan = summarize_text(user_api_key, raw_text)
                    if ringkasan.startswith("âŒ"):
                        st.error(ringkasan)
                    else:
                        st.success("âœ… Ringkasan berhasil dibuat!")
                        st.markdown("### ğŸ“ Ringkasan:")
                        st.write(ringkasan)

                        # Tombol unduh ringkasan sebagai TXT
                        filename = f"ringkasan_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                        st.download_button(
                            label="â¬‡ï¸ Unduh Ringkasan sebagai .txt",
                            data=ringkasan,
                            file_name=filename,
                            mime="text/plain"
                        )
else:
    st.warning("âš ï¸ Masukkan API Key terlebih dahulu untuk memulai.")
